% [1] Maeno T et al. 2007 PanDA: Distributed Production and Distributed Analysis System for ATLAS International Conference on Computing in High Energy and Nuclear Physics (Victoria, Canada)

@article{PanDA,
	author = {Maeno, T},
	year = {2008},
	month = {07},
	pages = {062036},
	title = {PanDA: Distributed production and distributed analysis system for ATLAS},
	volume = {119},
	journal = {Journal of Physics: Conference Series},
	doi = {10.1088/1742-6596/119/6/062036}
}

% [2] ATLAS Collaboration, JINST 3 S08003 (2008) 

@article{ATLAS_Collaboration_2008,
	doi = {10.1088/1748-0221/3/08/s08003},
	url = {https://doi.org/10.1088%2F1748-0221%2F3%2F08%2Fs08003},
	year = 2008,
	month = {aug},
	publisher = {{IOP} Publishing},
	volume = {3},
	number = {08},
	pages = {S08003--S08003},
	author = {G Aad and E Abat and J Abdallah and others},
	title = {The {ATLAS} Experiment at the {CERN} Large Hadron Collider},
	journal = {Journal of Instrumentation},
	annote = {The ATLAS detector as installed in its experimental cavern
	at point 1 at CERN is described in this paper. A brief overview of
	the expected performance of the detector when the Large Hadron
	Collider begins operation is also presented.}
}

% [3] 2005 LHC Computing Grid: Technical Design Report. document LCG-TDR-001, CERN-LHCC-2005-024 (The LCG TDR Editorial Board) ISBN 92-9083-253-3 

@article{Bird:2005js,
	author         = "Bird, I. and Bos, K. and Brook, N. and Duellmann, D. and
	Eck, C. and Fisk, I. and Foster, D. and Gibbard, B. and
	Grandi, C. and Grey, F. and others ",
	title          = "{LHC computing Grid. Technical design report}",
	year           = "2005",
	reportNumber   = "CERN-LHCC-2005-024",
	SLACcitation   = "%%CITATION = CERN-LHCC-2005-024;%%"
}

@article{doi:10.1146/annurev-nucl-102010-130059,
	author = {Bird, Ian},
	title = {Computing for the Large Hadron Collider},
	journal = {Annual Review of Nuclear and Particle Science},
	volume = {61},
	number = {1},
	pages = {99-118},
	year = {2011},
	abstract = { Following the first full year of Large Hadron Collider (LHC) data taking, the Worldwide LHC Computing Grid (WLCG) computing environment built to support LHC data processing and analysis has been validated. In this review, I discuss the rationale for the design of a distributed system and describe how this environment was constructed and deployed through the use of grid computing technologies. I discuss the experience with large-scale testing and operation with real accelerator data, which shows that expectations have been met and sometimes exceeded. The computing system's key achievements are that (a) the WLCG infrastructure is distributed and makes use of all the dispersed resources, (b) the experiments' computing models are also distributed and can make excellent use of the infrastructure, and (c) the computing system has enabled physics output in a very short time. Finally, I present prospects for the future evolution of the WLCG infrastructure. }
}



%[4] gLite web site:             		http://grid-deployment.web.cern.ch/grid-deployment/glite-web/

@electronic{gLite,
	title = {gLite web page},
	url={glite.cern.ch/}
}

%[5] DOE ASCR web page: https://science.energy.gov/ascr/

@electronic{DOEASCR,
	title = {{DOE ASCR} web page},
	url = {https://science.energy.gov/ascr/}
}

%[6] Titan at OLCF web page: https://www.olcf.ornl.gov/titan/ 

@electronic{Titan,
	title = {Titan web page},
	url = {https://www.olcf.ornl.gov/titan/}
}


@electronic{Summit,
	title = {Summit web page},
	url = {https://www.olcf.ornl.gov/summit/}
}


@electronic{NERSC_Cori,
	title = {Cori web page},
	url = {http://www.nersc.gov/users/computational-systems/cori/}
}


%[7] Xin Zhao et al.  PanDA Pilot Submission using Condor-G: Experience and Improvements.  2011 J. Phys.: Conf. Ser. 331 

@article{PanDAPilotSubmission,
	author = {Zhao, Xin and Hover, John and Wlodek, Tomasz and Wenaus, Torre and Frey, Jaime and Tannenbaum, Todd and Livny, Miron},
	year = {2011},
	month = {12},
	pages = {072069},
	title = {PanDA Pilot Submission using Condor-G: Experience and Improvements},
	volume = {331},
	journal = {Journal of Physics: Conference Series},
	doi = {10.1088/1742-6596/331/7/072069}
}

%[8] Nilsson P et al 2007 Experience from a pilot based system for ATLAS International Conference on Computing in High Energy and Nuclear Physics (Victoria, Canada)

@article{Nilsson_2008,
	doi = {10.1088/1742-6596/119/6/062038},
	url = {https://doi.org/10.1088%2F1742-6596%2F119%2F6%2F062038},
	year = 2008,
	month = {jul},
	publisher = {{IOP} Publishing},
	volume = {119},
	number = {6},
	pages = {062038},
	author = {P Nilsson},
	title = {Experience from a pilot based system for {ATLAS}},
	journal = {Journal of Physics: Conference Series},
	annote = {The PanDA software provides a highly performing distributed production and distributed analysis system. It is the first system in the ATLAS experiment [1] to use a pilot based late job delivery technique. This paper describes the architecture of the pilot system used in PanDA. Unique features have been implemented for high reliability automation in a distributed environment. Performance of PanDA is analyzed from one and a half years of experience of performing distributed computing on the Open Science Grid (OSG) infrastructure. Experience with pilot delivery mechanism using Condor-G [2], and a glide-in factory developed under OSG will be described.}
}

%[9] Harvester: https://github.com/PanDA WMS/panda-harvester/wiki 

@electronic{Harvester,
	title = {Harvester},
	url = {https://github.com/PanDAWMS/panda-harvester/wiki}
}

@article{Megino_2017,
	doi = {10.1088/1742-6596/898/5/052002},
	url = {https://doi.org/10.1088%2F1742-6596%2F898%2F5%2F052002},
	year = 2017,
	month = {oct},
	publisher = {{IOP} Publishing},
	volume = {898},
	pages = {052002},
	author = {F Megino and K De and A Klimentov and T Maeno and P Nilsson and D Oleynik and S Padolski and S Panitkin and T Wenaus},
	title = {{PanDA} for {ATLAS} distributed computing in the next decade},
	journal = {Journal of Physics: Conference Series},
	annote = {The Production and Distributed Analysis (PanDA) system has been developed to meet ATLAS production and analysis requirements for a data-driven workload management system capable of operating at the Large Hadron Collider (LHC) data processing scale. Heterogeneous resources used by the ATLAS experiment are distributed worldwide at hundreds of sites, thousands of physicists analyse the data remotely, the volume of processed data is beyond the exabyte scale, dozens of scientific applications are supported, while data processing requires more than a few billion hours of computing usage per year. PanDA performed very well over the last decade including the LHC Run 1 data taking period. However, it was decided to upgrade the whole system concurrently with the LHC’s first long shutdown in order to cope with rapidly changing computing infrastructure. After two years of reengineering efforts, PanDA has embedded capabilities for fully dynamic and flexible workload management. The static batch job paradigm was discarded in favor of a more automated and scalable model. Workloads are dynamically tailored for optimal usage of resources, with the brokerage taking network traffic and forecasts into account. Computing resources are partitioned based on dynamic knowledge of their status and characteristics. The pilot has been re-factored around a plugin structure for easier development and deployment. Bookkeeping is handled with both coarse and fine granularities for efficient utilization of pledged or opportunistic resources. An in-house security mechanism authenticates the pilot and data management services in off-grid environments such as volunteer computing and private local clusters. The PanDA monitor has been extensively optimized for performance and extended with analytics to provide aggregated summaries of the system as well as drill-down to operational details. There are as well many other challenges planned or recently implemented, and adoption by non-LHC experiments such as bioinformatics groups successfully running Paleomix (microbial genome and metagenomes) payload on supercomputers. In this paper we will focus on the new and planned features that are most important to the next decade of distributed computing workload management.}
}

%[10] L. S.Yung, C. Yang, X. Wan, W.Yu. GBOOST: A GPU-based tool for detecting gene-gene interactions in genome-wide case control studies. Bioinformatics, vol. 27, pp. 1309-10, March 2011.

%	journal = {Bioinformatics (Oxford, England)},

@article{GBOOST,
	author = {Yung, Ling Sing and Yang, Can and Wan, Xiang and Yu, Weichuan},
	year = {2011},
	month = {03},
	pages = {1309-10},
	title = {GBOOST: A GPU-based tool for detecting gene-gene interactions in genome-wide case control studies},
	volume = {27},
    journal = {Bioinformatics},	
	doi = {10.1093/bioinformatics/btr114}
}

%[11] Brooks B.R. et al. (29 July 2009). "CHARMM: The biomolecular simulation program". Journal of Computational Chemistry. 30 (10): 1545–1614. 

@article{3b6dad414e794d36954333f8f177f47c,
	title = "CHARMM: The biomolecular simulation program",
	abstract = "CHARMM (Chemistry at HARvard Molecular Mechanics) is a highly versatile and widely used molecular simulation program. It has been developed over the last three decades with a primary focus on molecules of biological interest, including proteins, peptides, lipids, nucleic acids, carbohydrates, and small molecule ligands, as they occur in solution, crystals, and membrane environments. For the study of such systems, the program, provides a large suite of computational tools that include numerous conformational and path sampling methods, free energy estimators, molecular minimization, dynamics, and analysis techniques, and model-building capabilities. The CHARMM! program is applicable to problems involving a much broader class of many-particle systems. Calculations with CHARMM can be performed, using a number of different energy functions and models, from mixed quantum mechanical-molecular mechanical force fields, to all-atom classical potential energy functions with explicit solvent and various boundary conditions, to implicit solvent and membrane models. The program, has been ported to numerous platforms in both serial and parallel architectures. This article provides an overview of the program, as it exists today with, an emphasis on developments since the publication of the original CHARMM article in 1983.",
	keywords = "Biomolecular simulation, Biophysical computation, CHARMM program, Energy function, Molecular dynamics, Molecular mechanics, Molecular modeling",
	author = "Brooks, {B. R.} and Brooks, {C. L.} and Mackerell, {A. D.} and others",
	year = "2009",
	month = "7",
	day = "30",
	doi = "10.1002/jcc.21287",
	language = "English (US)",
	volume = "30",
	pages = "1545--1614",
	journal = "Journal of Computational Chemistry",
	issn = "0192-8651",
	publisher = "John Wiley and Sons Inc.",
	number = "10",
}

%[12] Halzen, Francis and Klein, Spencer R. IceCube: An Instrument for Neutrino Astronomy. Rev. Sci. Instrum., vol.81, pp. 081101, 2010.

@article{doi:10.1063/1.3480478,
	author = {Halzen,Francis  and Klein,Spencer R. },
	title = {Invited Review Article: IceCube: An instrument for neutrino astronomy},
	journal = {Review of Scientific Instruments},
	volume = {81},
	number = {8},
	pages = {081101},
	year = {2010},
	doi = {10.1063/1.3480478}
}

%[13] A. Gazizov, M. P. Kowalski. ANIS: High Energy Neutrino Generator for Neutrino Telescopes.  Comput. Phys. Commun. 172 (2005) 203-213

@article{GAZIZOV2005203,
	title = "ANIS: High energy neutrino generator for neutrino telescopes",
	journal = "Computer Physics Communications",
	volume = "172",
	number = "3",
	pages = "203 - 213",
	year = "2005",
	issn = "0010-4655",
	doi = "https://doi.org/10.1016/j.cpc.2005.03.113",
	url = "http://www.sciencedirect.com/science/article/pii/S0010465505004194",
	author = "A. Gazizov and M. Kowalski",
	keywords = "High energy neutrinos, Monte Carlo event generator, Neutrino telescopes, Astroparticle physics, Neutrino propagation, Neutrino interaction",
	abstract = "We present the high-energy neutrino Monte Carlo event generator ANIS (All Neutrino Interaction Simulation). The program provides a detailed and flexible neutrino event simulation for high-energy neutrino detectors, such as AMANDA, ANTARES or ICECUBE. It generates neutrinos of any flavor according to a specified flux and propagates them through the Earth. In a final step neutrino interactions are simulated within a specified volume. All relevant standard model processes are implemented. We discuss strengths and limitations of the program.
	Program summary
	Title of program: ANIS Catalogue identifier:ADWF Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADWF Program is obtainable from: CPC Program Library, Queen's University of Belfast, N. Ireland Computer on which the program has been thoroughly tested: Intel-Pentium based Personal Computers Operating system:Linux Programming language used:C++ Memory required to execute:13 megabyte Number of lines in distributed program, including test data, etc.:912 424 Number of bytes in distributed program, including test data, etc.: 6 876 631 Distribution format:tar.gz Libraries used by ANIS:HepMC [M. Dobbs, J.B. Hansen, Comput. Phys. Comm. 134 (2001) 41], CLHEP vector package [http://wwwinfo.cern.ch/asd/lhc++/clhep] Nature of physical problem:Monte Carlo neutrino event generator for high-energy neutrino telescopes. Method of solution:Neutrino events are first sampled according a specified flux, then propagated through the Earth and finally are allowed to interact inside a detection volume. Restrictions of the program:Neutrino energies range from 10 to 1012 GeV. Typical running time:104 events require typically a 1-GHz CPU time of about 300 s."
}

%[14] Globus GridFTP: http://toolkit.globus.org/toolkit/docs/latest-stable/gridftp/

@electronic{GlobusGridFTP,
	title = {Globus GridFTP web page},
	url = {http://toolkit.globus.org/toolkit/docs/latest-stable/gridftp/}
}

@inproceedings{GridFTP_Proc, 
	author={W. Allcock and J. Bresnahan and R. Kettimuthu and M. Link}, 
	booktitle={SC '05: Proceedings of the 2005 ACM/IEEE Conference on Supercomputing}, 
	title={The Globus Striped GridFTP Framework and Server}, 
	year={2005}, 
	volume={}, 
	number={}, 
	pages={54-54}, 
	keywords={Protocols;Network servers;Government;Wide area networks;Network interfaces;File systems;Robustness;Grid computing;Machinery;Libraries}, 
	doi={10.1109/SC.2005.72}, 
	ISSN={}, 
	month={Nov}}

%[15] H.Markram: The blue brain project. Nat Rev Neurosci. 7, pp. 153-160, 2006

@article{Markram:BBP,
	Author = {Markram, Henry},
	Date = {2006/02/01/online},
	Date-Added = {2019-01-25 22:40:50 +0000},
	Date-Modified = {2019-01-25 22:40:50 +0000},
	Day = {01},
	Journal = {Nature Reviews Neuroscience},
	L3 = {10.1038/nrn1848; },
	M3 = {Perspective},
	Month = {02},
	Pages = {153 EP  -},
	Publisher = {Nature Publishing Group SN  -},
	Title = {The Blue Brain Project},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nrn1848},
	Volume = {7},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1038/nrn1848}}


%[16] LSST DESC web page: http://www.lsst-desc.org

@electronic{lsst-desc,
	title = {{LSST DESC} web page},
	url = {http://www.lsst-desc.org}
}

%[17] Ronald Babich, Michael A. Clark, and Bálint Joó. 2010. Parallelizing the QUDA Library for Multi-GPU Calculations in Lattice Quantum Chromodynamics. In Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis (SC '10). IEEE Computer Society, Washington, DC, USA, pp. 1-11.

@inproceedings{Babich:2010:PQL:1884643.1884695,
	author = {Babich, Ronald and Clark, Michael A. and Jo\'{o}, B\'{a}lint},
	title = {Parallelizing the QUDA Library for Multi-GPU Calculations in Lattice Quantum Chromodynamics},
	booktitle = {Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis},
	series = {SC '10},
	year = {2010},
	pages = {1--11},
	numpages = {11},
	acmid = {1884695},
	publisher = {IEEE Computer Society},
	address = {Washington, DC, USA},
} 

@inproceedings{Babich:2011np,
	author         = "Babich, R. and Clark, M. A. and Joo, B. and others",
	title          = "{Scaling Lattice QCD beyond 100 GPUs}",
	booktitle      = "{SC11 International Conference for High Performance
	Computing, Networking, Storage and Analysis Seattle,
	Washington, November 12-18, 2011}",
	year           = "2011",
	archivePrefix  = "arXiv",
	primaryClass   = "hep-lat",
	SLACcitation   = "%%CITATION = ARXIV:1109.2935;%%"
}


%[18] Globus data transfer web site: https://www.globus.org/data-transfer

@electronic{Globusdatatransfer,
	title = {Globus data transfer web page},
	url = {https://www.globus.org/data-transfer}
}

%[19] S. Lamoreaux and R. Golub. Experimental searches for the neutron electric dipole moment. Journal of Physics G: Nuclear and Particle Physics, vol. 36, num. 10, pp. 104002

@article{Lamoreaux_2009,
	doi = {10.1088/0954-3899/36/10/104002},
	url = {https://doi.org/10.1088%2F0954-3899%2F36%2F10%2F104002},
	year = 2009,
	month = {sep},
	publisher = {{IOP} Publishing},
	volume = {36},
	number = {10},
	pages = {104002},
	author = {S K Lamoreaux and R Golub},
	title = {Experimental searches for the neutron electric dipole moment},
	journal = {Journal of Physics G: Nuclear and Particle Physics},
	annote = {The possible existence of a neutron electric dipole (EDM) was put forward as an experimental question in 1949, 60 years ago, and still remains an outstanding question in modern physics. A review of the technical innovations that allowed for improving the experimental limit by nearly eight orders of magnitude (approximately a decade per decade) will be presented, along with a discussion of the prospects for further improvement.}
}

@article{AGOSTINELLI2003250,
	title = "Geant4 -- a simulation toolkit",
	journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
	volume = "506",
	number = "3",
	pages = "250 - 303",
	year = "2003",
	issn = "0168-9002",
	doi = "https://doi.org/10.1016/S0168-9002(03)01368-8",
	url = "http://www.sciencedirect.com/science/article/pii/S0168900203013688",
	author = "S. Agostinelli and J. Allison and K. Amako and others",
	keywords = "Simulation, Particle interactions, Geometrical modelling, Software engineering, Object-oriented technology, Distributed software development",
	abstract = "Geant4 is a toolkit for simulating the passage of particles through matter. It includes a complete range of functionality including tracking, geometry, physics models and hits. The physics processes offered cover a comprehensive range, including electromagnetic, hadronic and optical processes, a large set of long-lived particles, materials and elements, over a wide energy range starting, in some cases, from 250eV and extending in others to the TeV energy range. It has been designed and constructed to expose the physics models utilised, to handle complex geometries, and to enable its easy adaptation for optimal use in different sets of applications. The toolkit is the result of a worldwide collaboration of physicists and software engineers. It has been created exploiting software engineering and object-oriented technology and implemented in the C++ programming language. It has been used in applications in particle physics, nuclear physics, accelerator design, space engineering and medical physics."
}

@article{ALLISON2016186,
	title = "Recent developments in Geant4",
	journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
	volume = "835",
	pages = "186 - 225",
	year = "2016",
	issn = "0168-9002",
	doi = "https://doi.org/10.1016/j.nima.2016.06.125",
	url = "http://www.sciencedirect.com/science/article/pii/S0168900216306957",
	author = "J. Allison and K. Amako and J. Apostolakis and P. Arce and M. Asai and T. Aso and E. Bagli and A. Bagulya and S. Banerjee and G. Barrand and B.R. Beck and A.G. Bogdanov and D. Brandt and J.M.C. Brown and H. Burkhardt and Ph. Canal and D. Cano-Ott and S. Chauvie and K. Cho and G.A.P. Cirrone and G. Cooperman and M.A. Cortés-Giraldo and G. Cosmo and G. Cuttone and G. Depaola and L. Desorgher and X. Dong and A. Dotti and V.D. Elvira and G. Folger and Z. Francis and A. Galoyan and L. Garnier and M. Gayer and K.L. Genser and V.M. Grichine and S. Guatelli and P. Guèye and P. Gumplinger and A.S. Howard and I. Hřivnáčová and S. Hwang and S. Incerti and A. Ivanchenko and V.N. Ivanchenko and F.W. Jones and S.Y. Jun and P. Kaitaniemi and N. Karakatsanis and M. Karamitros and M. Kelsey and A. Kimura and T. Koi and H. Kurashige and A. Lechner and S.B. Lee and F. Longo and M. Maire and D. Mancusi and A. Mantero and E. Mendoza and B. Morgan and K. Murakami and T. Nikitina and L. Pandola and P. Paprocki and J. Perl and I. Petrović and M.G. Pia and W. Pokorski and J.M. Quesada and M. Raine and M.A. Reis and A. Ribon and A. Ristić Fira and F. Romano and G. Russo and G. Santin and T. Sasaki and D. Sawkey and J.I. Shin and I.I. Strakovsky and A. Taborda and S. Tanaka and B. Tomé and T. Toshito and H.N. Tran and P.R. Truscott and L. Urban and V. Uzhinsky and J.M. Verbeke and M. Verderi and B.L. Wendt and H. Wenzel and D.H. Wright and D.M. Wright and T. Yamashita and J. Yarba and H. Yoshida",
	keywords = "High energy physics, Nuclear physics, Radiation, Simulation, Computing",
	abstract = "Geant4 is a software toolkit for the simulation of the passage of particles through matter. It is used by a large number of experiments and projects in a variety of application domains, including high energy physics, astrophysics and space science, medical physics and radiation protection. Over the past several years, major changes have been made to the toolkit in order to accommodate the needs of these user communities, and to efficiently exploit the growth of computing power made available by advances in technology. The adaptation of Geant4 to multithreading, advances in physics, detector modeling and visualization, extensions to the toolkit, including biasing and reverse Monte Carlo, and tools for physics and release validation are discussed here."
}


@article{GridPP_Collaboration_2005,
	doi = {10.1088/0954-3899/32/1/n01},
	url = {https://doi.org/10.1088%2F0954-3899%2F32%2F1%2Fn01},
	year = 2005,
	month = {nov},
	publisher = {{IOP} Publishing},
	volume = {32},
	number = {1},
	pages = {N1--N20},
	author = {P J W Faulkner and L S Lowe and C L A Tan and others},
	title = {{GridPP}: development of the {UK} computing Grid for particle physics},
	journal = {Journal of Physics G: Nuclear and Particle Physics},
	annote = {The GridPP Collaboration is building a UK computing Grid for particle physics, as part of the international effort towards computing for the Large Hadron Collider. The project, funded by the UK Particle Physics and Astronomy Research Council (PPARC), began in September 2001 and completed its first phase 3 years later. GridPP is a collaboration of approximately 100 researchers in 19 UK university particle physics groups, the Council for the Central Laboratory of the Research Councils and CERN, reflecting the strategic importance of the project. In collaboration with other European and US efforts, the first phase of the project demonstrated the feasibility of developing, deploying and operating a Grid-based computing system to meet the UK needs of the Large Hadron Collider experiments. This note describes the work undertaken to achieve this goal.}
}

@article{Pordes_2007,
	doi = {10.1088/1742-6596/78/1/012057},
	url = {https://doi.org/10.1088%2F1742-6596%2F78%2F1%2F012057},
	year = 2007,
	month = {jul},
	publisher = {{IOP} Publishing},
	volume = {78},
	pages = {012057},
	author = {Ruth Pordes and Miron Livny and Torre Wenaus and others},
	title = {The open science grid},
	journal = {Journal of Physics: Conference Series},
	annote = {The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared & common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid.org.}
}


@electronic{LQCD_Matsufuru,
	title = {Introduction to lattice QCD simulations},
	author = {Hideo Matsufuru},
	url = {http://research.kek.jp/people/matufuru/Research/Docs/Lattice/Introduction/note_lattice.pdf}
}


@article {Gros277,
	author = {Gros, Pierre-Alexis and Le Nagard, Herv{\'e} and Tenaillon, Olivier},
	title = {The Evolution of Epistasis and Its Links With Genetic Robustness, Complexity and Drift in a Phenotypic Model of Adaptation},
	volume = {182},
	number = {1},
	pages = {277--293},
	year = {2009},
	doi = {10.1534/genetics.108.099127},
	publisher = {Genetics},
	abstract = {The epistatic interactions among mutations have a large effect on the evolution of populations. In this article we provide a formalism under which epistatic interactions among pairs of mutations have a distribution whose mean can be modulated. We find that the mean epistasis is correlated to the effect of mutations or genetic robustness, which suggests that such formalism is in good agreement with most in silico models of evolution where the same pattern is observed. We further show that the evolution of epistasis is highly dependant on the intensity of drift and of how complex the organisms are, and that either positive or negative epistasis could be selected for, depending on the balance between the efficiency of selection and the intensity of drift.},
	issn = {0016-6731},
	journal = {Genetics}
}
